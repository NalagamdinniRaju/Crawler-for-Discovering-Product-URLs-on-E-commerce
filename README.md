

# ğŸ›ï¸ E-commerce Product Crawler

![Node.js](https://img.shields.io/badge/Node.js-43853D?style=for-the-badge&logo=node.js&logoColor=white)
![JavaScript](https://img.shields.io/badge/JavaScript-F7DF1E?style=for-the-badge&logo=javascript&logoColor=black)
![Puppeteer](https://img.shields.io/badge/Puppeteer-40B5A4?style=for-the-badge&logo=puppeteer&logoColor=white)
![NPM](https://img.shields.io/badge/NPM-%23CB3837.svg?style=for-the-badge&logo=npm&logoColor=white)

## ğŸ“– Overview
A powerful and configurable web crawler designed specifically for e-commerce websites. This crawler efficiently discovers and extracts product URLs from major e-commerce platforms while respecting rate limits and robots.txt protocols.

## âœ¨ Features
- ğŸŒ Multi-domain crawling support
- âš™ï¸ Configurable crawling parameters
- ğŸ¤– Respect for robots.txt
- ğŸš¦ Rate limiting and concurrent request management
- ğŸ”„ Automatic retry mechanism
- ğŸ“ Comprehensive logging system
- ğŸ¯ Domain-specific product URL pattern matching
- ğŸš« Exclusion patterns for non-product pages
- ğŸ’¾ Results persistence in JSON format

## ğŸ›’ Supported E-commerce Platforms
- Amazon
- Flipkart
- Myntra
- Walmart
- Best Buy
- Target
- eBay
- AliExpress
- Etsy
- Shopify
- Zappos
- Saks Fifth Avenue

## ğŸ“‹ Prerequisites
- Node.js (v14 or higher)
- npm or yarn

## ğŸš€ Installation
```bash
# Clone the repository
git clone https://github.com/yourusername/ecommerce-crawler.git

# Install dependencies
cd ecommerce-crawler
npm install
```

## âš™ï¸ Configuration
Create a `.env` file in the root directory:
```env
NODE_ENV=development
MAX_CONCURRENT_CRAWLS=5
CRAWLER_TIMEOUT=30000
```

## ğŸ“‚ Project Structure
```
ecommerce-crawler/
â”œâ”€â”€ ğŸ“ node_modules/
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ ğŸ“ config/
â”‚   â”‚   â””â”€â”€ ğŸ“„ config.js
â”‚   â”œâ”€â”€ ğŸ“ controllers/
â”‚   â”‚   â””â”€â”€ ğŸ“„ crawlerController.js
â”‚   â”œâ”€â”€ ğŸ“ services/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ crawlerService.js
â”‚   â”‚   â””â”€â”€ ğŸ“„ urlParser.js
â”‚   â””â”€â”€ ğŸ“ utils/
â”‚       â”œâ”€â”€ ğŸ“„ logger.js
â”‚       â””â”€â”€ ğŸ“„ validators.js
â”œâ”€â”€ ğŸ“ logs/
â”œâ”€â”€ ğŸ“ output/
â”œâ”€â”€ ğŸ“„ .env
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ index.js
â”œâ”€â”€ ğŸ“„ package-lock.json
â”œâ”€â”€ ğŸ“„ package.json
â””â”€â”€ ğŸ“„ README.md
```

## ğŸ”§ Usage
```bash
# Start the crawler
npm start

```

## ğŸ“Š Logging
Logs are stored in the `logs` directory:
- ğŸ“„ error.log: Contains error-level logs
- ğŸ“„ combined.log: Contains all logs

## ğŸ’¾ Output
Crawled URLs are saved in `output/results.json`


## ğŸ“ Environment Variables
```env
NODE_ENV=development        # Application environment
MAX_CONCURRENT_CRAWLS=5     # Maximum concurrent crawl operations
CRAWLER_TIMEOUT=30000       # Timeout in milliseconds
```

## ğŸš€ API Reference

### CrawlerController
```javascript
const crawler = new CrawlerController();
await crawler.crawlDomains(['www.example.com']);
```

### URLParser
```javascript
const parser = new URLParser();
parser.isProductUrl('https://www.example.com/product/123');
```

## ğŸ“ˆ Performance
- Configurable concurrent requests
- Built-in rate limiting
- Memory-efficient crawling
- Automatic retry mechanism

## ğŸ›¡ï¸ Error Handling
- Comprehensive error logging
- Automatic retry for failed requests
- Graceful shutdown mechanism

## ğŸ™ Acknowledgments
- [Puppeteer](https://pptr.dev/)
- [Winston Logger](https://github.com/winstonjs/winston)
- [dotenv](https://github.com/motdotla/dotenv)
